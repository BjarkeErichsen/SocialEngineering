{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18fb44e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==any\n",
      "  Downloading https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl (12.8 MB)\n",
      "     --------------------------------------- 12.8/12.8 MB 20.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from en-core-web-sm==any) (3.5.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (1.0.9)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (2.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (6.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (1.24.1)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (22.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (2.4.5)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (3.0.12)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (8.1.7)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (3.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (65.6.3)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (3.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (1.10.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (4.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (3.0.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (0.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\bbjar\\anaconda3\\envs\\compscienv\\lib\\site-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-sm==any) (2.1.1)\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time \n",
    "from transformers import pipeline\n",
    "import en_core_web_sm\n",
    "\n",
    "!pip install https://huggingface.co/spacy/en_core_web_sm/resolve/main/en_core_web_sm-any-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd39740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "716\n"
     ]
    }
   ],
   "source": [
    "LINK2 = \"https://easychair.org/smart-program/IC2S2-2021/talk_author_index.html\"\n",
    "r = requests.get(LINK2)\n",
    "soup2021 = BeautifulSoup(r.content)\n",
    "participants2021 = []\n",
    "for i in range(2000):\n",
    "    participants2021.append(soup2021.find_all(href = \"person\"+str(i)+\".html\"))\n",
    "participants2021_text = []\n",
    "for name in participants2021:\n",
    "    if len(name) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        participants2021_text.append(name[0].get_text())\n",
    "\n",
    "print(len(participants2021_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a54b7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1598\n",
      "['Ziv Epstein11:00 –', 'Ding', 'Jingwen Zhang', 'Vaccine Misinformation', 'Juhi Kulshrestha', 'Abhijnan Chakraborty', 'Meeyoung Cha', 'Krishna Gummadi', 'Analyzing Biases', 'Alexandre Bovet']\n",
      "['Frank Peter Pijpers', 'Martin Lukac', 'André Grow', 'Kiran Garimella', 'Gianmarco De Francisci Morales', 'Aristides Gionis', 'Michael Mathioudakis', 'Eaman Jahani', 'Peter Krafft', 'Yoshihiko Suhara']\n"
     ]
    }
   ],
   "source": [
    "LINK = \"https://2019.ic2s2.org/delegates/\"\n",
    "r = requests.get(LINK)\n",
    "soup2019 = BeautifulSoup(r.content)\n",
    "participants2019 = soup2019.find_all('td')\n",
    "participants2019 = participants2019[2:]\n",
    "participants2019 = participants2019[::2]\n",
    "participants2019_text = []\n",
    "for name in participants2019:\n",
    "    participants2019_text.append(name.get_text())\n",
    "    \n",
    "participants2019_DELEGATES_text = participants2019_text.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Link2019Posters = \"https://2019.ic2s2.org/posters/\"\n",
    "LINK = Link2019Posters\n",
    "r = requests.get(LINK)\n",
    "soup2019 = BeautifulSoup(r.content)\n",
    "soupLI = soup2019.find_all('li')\n",
    "participants_POSTERLINK_text_2019 = []\n",
    "for i in range(32, len(soupLI)):\n",
    "    try:\n",
    "        helper = soupLI[i].next_element\n",
    "        And_names = helper.split(\"and\")\n",
    "\n",
    "        if len(And_names) > 1:\n",
    "            comma_names = And_names[0].split(\",\")\n",
    "\n",
    "            comma_names.append(And_names[1])\n",
    "            #comma_names.extend(And_names[1])\n",
    "\n",
    "            total_names = comma_names.copy()\n",
    "        else:\n",
    "            total_names = And_names.copy()\n",
    "        \n",
    "        \n",
    "        for name in total_names:\n",
    "            if name[0] == \" \":\n",
    "                name = name[1:]\n",
    "            if name[-1] == \" \":\n",
    "                name = name[:-1]\n",
    "\n",
    "            participants_POSTERLINK_text_2019.append(name)\n",
    "    except: \n",
    "        pass \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "nlp = en_core_web_sm.load()\n",
    "def intelligent_name_collection(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    names_of_presenters = []\n",
    "    name = \"\"\n",
    "    doing_a_name = False\n",
    "    ready_for_new_name = True\n",
    "    for token in doc:    \n",
    "\n",
    "        if token.ent_type_ == \"PERSON\":\n",
    "            if ready_for_new_name:\n",
    "                name = token.text\n",
    "                ready_for_new_name = False\n",
    "                doing_a_name = True\n",
    "            else:\n",
    "                name = name + \" \" + token.text\n",
    "\n",
    "        if token.ent_type_ != \"PERSON\":\n",
    "\n",
    "            if doing_a_name:\n",
    "                names_of_presenters.append(name)\n",
    "\n",
    "                doing_a_name = False\n",
    "                ready_for_new_name = True\n",
    "    \n",
    "    return names_of_presenters\n",
    "\n",
    "\n",
    "Link2019Presentations = \"https://2019.ic2s2.org/oral-presentations/\"\n",
    "LINK = Link2019Presentations\n",
    "r = requests.get(LINK)\n",
    "soup2019 = BeautifulSoup(r.content)\n",
    "soup = soup2019.find_all('p')\n",
    "\n",
    "participants_PRESENTATIONS_text_2019 = []\n",
    "\n",
    "for e in range(3, len(soup)):\n",
    "    \n",
    "    split_text = soup[e].get_text()\n",
    "    \n",
    "    names_of_presenters_gathered = intelligent_name_collection(split_text)\n",
    "    \n",
    "    participants_PRESENTATIONS_text_2019.extend(names_of_presenters_gathered)\n",
    "    \n",
    "\n",
    "\n",
    "participants2019_text = []\n",
    "participants2019_text.extend(participants2019_DELEGATES_text)\n",
    "participants2019_text.extend(participants_POSTERLINK_text_2019)\n",
    "participants2019_text.extend(participants_PRESENTATIONS_text_2019)\n",
    "\n",
    "print(len(participants2019_text))\n",
    "\n",
    "print(participants_PRESENTATIONS_text_2019[0:10])\n",
    "print(participants_POSTERLINK_text_2019[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "273e8c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1389\n"
     ]
    }
   ],
   "source": [
    "LINK3 = \"https://docs.google.com/spreadsheets/u/0/d/e/2PACX-1vTX9_1Xftn7D-nSI8X9b7tafr_Z0kAbphKdfZ8qUSU9p-syXNsGPdhHl5ZyTnKKL-T6dCEJqtsrn3wy/pubhtml/sheet?headers=false&gid=181378784\"\n",
    "\n",
    "tables = pd.read_html(LINK3, header=1)\n",
    "Presenters = tables[0][\"Presenters\"]\n",
    "\n",
    "participants_2020 = []\n",
    "for i in range(len(Presenters)):\n",
    "    if type(Presenters[i]) == str:\n",
    "        for name in Presenters[i].split(\",\"):\n",
    "           participants_2020.append(name)\n",
    "\n",
    "print(len(participants_2020))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f374c93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       Abbasiharofteh, Milad\n",
       "1                 Adam, Silke\n",
       "2           Aggarwal, Anupama\n",
       "3          Aiello, Luca Maria\n",
       "4       Akbaritabar, Aliakbar\n",
       "                ...          \n",
       "3698          Kreuter, Frauke\n",
       "3699          Mai, Tung-Duong\n",
       "3700            Neffke, Frank\n",
       "3701        Secchini, Valeria\n",
       "3702          Thurner, Stefan\n",
       "Name: Names, Length: 3703, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "participantsALL_text = []\n",
    "participantsALL_text.extend(participants2019_text.copy())\n",
    "participantsALL_text.extend(participants_2020.copy())\n",
    "participantsALL_text.extend(participants2021_text.copy())\n",
    "\n",
    "pandasNames = pd.DataFrame(participantsALL_text, columns=[\"Names\"])\n",
    "\n",
    "\n",
    "print(len(pandasNames[\"Names\"]))\n",
    "pandasNames.drop_duplicates()\n",
    "pandasNames[\"Names\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c3ae3",
   "metadata": {},
   "source": [
    "Day 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4933c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "26\n",
      "1\n",
      "612\n",
      "2\n",
      "616\n",
      "3\n",
      "1187\n",
      "4\n",
      "1217\n",
      "5\n",
      "1223\n",
      "6\n",
      "1263\n",
      "7\n",
      "1303\n",
      "8\n",
      "1402\n",
      "9\n",
      "1410\n",
      "10\n",
      "1424\n",
      "11\n",
      "1445\n",
      "12\n",
      "1627\n",
      "13\n",
      "failed:  13\n",
      "14\n",
      "1638\n",
      "15\n",
      "1649\n",
      "16\n",
      "1652\n",
      "17\n",
      "1675\n",
      "18\n",
      "1848\n",
      "19\n",
      "1874\n",
      "20\n",
      "2233\n",
      "21\n",
      "2439\n",
      "22\n",
      "2445\n",
      "23\n",
      "failed:  23\n",
      "24\n",
      "2594\n",
      "25\n",
      "3113\n",
      "26\n",
      "3137\n",
      "27\n",
      "failed:  27\n",
      "28\n",
      "3186\n",
      "29\n",
      "3192\n",
      "30\n",
      "failed:  30\n",
      "31\n",
      "3201\n",
      "32\n",
      "failed:  32\n",
      "33\n",
      "3359\n",
      "34\n",
      "3459\n",
      "35\n",
      "3471\n",
      "36\n",
      "failed:  36\n",
      "37\n",
      "4029\n",
      "38\n",
      "4035\n",
      "39\n",
      "4056\n",
      "40\n",
      "4058\n",
      "41\n",
      "4086\n",
      "42\n",
      "4190\n",
      "43\n",
      "4198\n",
      "44\n",
      "4207\n",
      "45\n",
      "4230\n",
      "46\n",
      "4240\n",
      "47\n",
      "failed:  47\n",
      "48\n",
      "4320\n",
      "49\n",
      "4384\n",
      "50\n",
      "4427\n",
      "51\n",
      "4594\n",
      "52\n",
      "failed:  52\n",
      "53\n",
      "4776\n",
      "54\n",
      "4785\n",
      "55\n",
      "5111\n",
      "56\n",
      "7036\n",
      "57\n",
      "8066\n",
      "58\n",
      "8320\n",
      "59\n",
      "8343\n",
      "60\n",
      "8407\n",
      "61\n",
      "8413\n",
      "62\n",
      "8476\n",
      "63\n",
      "failed:  63\n",
      "64\n",
      "8596\n",
      "65\n",
      "8927\n",
      "66\n",
      "8955\n",
      "67\n",
      "failed:  67\n",
      "68\n",
      "8968\n",
      "69\n",
      "9035\n",
      "70\n",
      "9103\n",
      "71\n",
      "9234\n",
      "72\n",
      "failed:  72\n",
      "73\n",
      "9522\n",
      "74\n",
      "9525\n",
      "75\n",
      "9851\n",
      "76\n",
      "9941\n"
     ]
    }
   ],
   "source": [
    "\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/search\"   #author or paper\n",
    "\n",
    "authorIDs_list = []\n",
    "authorColabIDs_list = []\n",
    "i = 0\n",
    "loop_fails = 0\n",
    "author_count = 0\n",
    "author_colab_count = 0\n",
    "for name in pandasNames[\"Names\"]:\n",
    "    print(i)\n",
    "    \n",
    "    params = {'query':name,     #name, because we search by author\n",
    "               \"offset\":0,               #offset dunno\n",
    "               \"limit\":1,                   #limit is not neccessary but gives the number\n",
    "                \"fields\":\"papers.authors\"}      #we can search for any of the fields given at semantic scholar https://api.semanticscholar.org/api-docs/graph#tag/Paper-Data/operation/get_graph_get_paper_references\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        my_url = BASE_URL + VERSION + RESOURCE\n",
    "        r = requests.get(my_url, params=params)\n",
    "        json_file = r.json()[\"data\"]\n",
    "        \n",
    "        authorId = json_file[0][\"authorId\"]\n",
    "        author_count += 1\n",
    "        \n",
    "        \n",
    "        for paper_i in range(len(json_file[0][\"papers\"])):\n",
    "            for auth in range(len(json_file[0][\"papers\"][paper_i][\"authors\"])):\n",
    "                try:\n",
    "                    colab_id = json_file[0][\"papers\"][paper_i][\"authors\"][auth][\"authorId\"]\n",
    "                    authorColabIDs_list.append(colab_id)\n",
    "                    author_colab_count += 1\n",
    "                except:\n",
    "                    pass\n",
    "        authorIDs_list.append(authorId)\n",
    "\n",
    "        print(author_colab_count)\n",
    "    except:\n",
    "        print(\"failed: \", i)\n",
    "        loop_fails += 1\n",
    "    \n",
    "    time.sleep(5*60 / 100 + 0.1)\n",
    "    \n",
    "    i+=1\n",
    "    \n",
    "    \n",
    "with open(r'C:\\Users\\Bbjar\\OneDrive\\Skrivebord\\Semester6\\Comp Social Science\\AuthorID.txt', 'w') as fp:\n",
    "    for item in authorIDs_list:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "\n",
    "with open(r'C:\\Users\\Bbjar\\OneDrive\\Skrivebord\\Semester6\\Comp Social Science\\AuthorColabsID.txt', 'w') as fp:\n",
    "    for item in authorColabIDs_list:\n",
    "        # write each item on a new line\n",
    "        fp.write(\"%s\\n\" % item)\n",
    "\n",
    "        \n",
    "\n",
    "    #100 per 5 minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b8d4ad",
   "metadata": {},
   "source": [
    "Consider the list of author ids you have found in the exercise above. For each author, use the Academic Graph API to find:\n",
    "\n",
    "their aliases\n",
    "their name\n",
    "their papers, where for each paper we want to retain:\n",
    "title\n",
    "abstract\n",
    "the year of publication\n",
    "the externalIds (this is because there are universal identifiers for scientific works called DOI that we can use across platforms)\n",
    "s2FieldsOfStudy the fields of study\n",
    "citationCount the number of times that this paper was cited\n",
    "(Hint: you can find authors in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3fbd0376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91254\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file1 = open(r'C:\\Users\\Bbjar\\OneDrive\\Skrivebord\\Semester6\\Comp Social Science\\AuthorID.txt', 'r')\n",
    "Lines1 = file1.readlines()\n",
    "authorID = []\n",
    "\n",
    "file2 = open(r'C:\\Users\\Bbjar\\OneDrive\\Skrivebord\\Semester6\\Comp Social Science\\AuthorColabsID.txt')\n",
    "Lines2 = file2.readlines()\n",
    "authorColabsID = []\n",
    "\n",
    "for line in Lines1:\n",
    "    authorID.append(line[:-1])\n",
    "for line in Lines2:\n",
    "    authorColabsID.append(line[:-1])\n",
    "\n",
    "\n",
    "all_authers_list = authorColabsID.copy()\n",
    "all_authers_list.extend(authorID)\n",
    "all_authers_list = list(set(all_authers_list))\n",
    "print(len(all_authers_list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd59f3ae",
   "metadata": {},
   "source": [
    "From adrian "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "2d62ed2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def author_df(data):\n",
    "    helperdict1 = {'authorId':[],'name':[], 'aliases':[], 'citationCount':[], 'field':[]}\n",
    "\n",
    "    for person_index in range(len(data)):\n",
    "        authorId = data[person_index]['authorId']\n",
    "        name  = data[person_index]['name']\n",
    "        aliases  =  data[person_index]['aliases']\n",
    "        helperFields_momentary = []\n",
    "        citation_count = 0\n",
    "        for paper_index in range(len(data[person_index]['papers'])):\n",
    "            try:\n",
    "                s2FieldsOfStudy = data[person_index]['papers'][paper_index]['s2FieldsOfStudy'][0][\"category\"]\n",
    "                helperFields_momentary.append(s2FieldsOfStudy)\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "            citation_count += data[person_index]['papers'][paper_index]['citationCount']\n",
    "        try:\n",
    "            the_mode = mode(helperFields_momentary)\n",
    "        except:\n",
    "            the_mode = None\n",
    "        helperdict1['field'].append(the_mode)\n",
    "        helperdict1['citationCount'].append(citation_count)\n",
    "        helperdict1['authorId'].append(authorId)\n",
    "        helperdict1['name'].append(name)\n",
    "        helperdict1['aliases'].append(aliases)\n",
    "        \n",
    "    return pd.DataFrame(helperdict1)\n",
    "\n",
    "def paper_df(data):\n",
    "    helperdict2 = {'paperId':[],'title':[], 'year':[], 'externalId':[], 'citationCount':[], 'fields':[], \"authorIds\":[]}\n",
    "\n",
    "    for person_index in range(len(data)):    \n",
    "        for paper_index in range(len(data[person_index]['papers'])):\n",
    "            try:\n",
    "                paperId = data[person_index]['papers'][paper_index]['paperId']\n",
    "                title = data[person_index]['papers'][paper_index]['title']\n",
    "                year  = data[person_index]['papers'][paper_index]['year']\n",
    "                doi = data[person_index]['papers'][paper_index]['externalIds']['DOI']\n",
    "                citationCount = data[person_index]['papers'][paper_index]['citationCount']\n",
    "                \n",
    "                helper1 = []\n",
    "                for i in range(len(data[person_index]['papers'][paper_index]['s2FieldsOfStudy'])):\n",
    "                    helper1.append(data[person_index]['papers'][paper_index]['s2FieldsOfStudy'][i][\"category\"])\n",
    "\n",
    "                helper2 = []\n",
    "                for i in range(len(data[person_index]['papers'][paper_index]['authors'])):\n",
    "                    helper2.append(data[person_index]['papers'][paper_index]['authors'][i]['authorId'])\n",
    "                \n",
    "                \n",
    "                def fail_if_other_dataframe_would_fail():\n",
    "                    #this cuts of around 15% of the data as we see through testing\n",
    "\n",
    "                    #This is to make the statement fail if abstract dataframe would also fail\n",
    "                    data[person_index]['papers'][paper_index]['abstract']\n",
    "                fail_if_other_dataframe_would_fail()\n",
    "                \n",
    "                #appending\n",
    "                helperdict2['paperId'].append(paperId)\n",
    "                helperdict2['title'].append(title)\n",
    "                helperdict2['year'].append(year)\n",
    "                helperdict2['externalId'].append(doi)\n",
    "                helperdict2['citationCount'].append(citationCount)\n",
    "                helperdict2[\"fields\"].append(helper1)\n",
    "                helperdict2[\"authorIds\"].append(helper2)\n",
    "            except:\n",
    "                pass \n",
    "\n",
    "    return pd.DataFrame(helperdict2)\n",
    "\n",
    "def abstract_df(data):\n",
    "    helperdict3 = {'paperId':[],'abstract':[]}\n",
    "    g = 0\n",
    "    for person_index in range(len(data)):\n",
    "        \n",
    "        for paper_index in range(len(data[person_index]['papers'])):\n",
    "            \n",
    "            g += 1\n",
    "            try:\n",
    "                \n",
    "                def fail_if_other_dataframe_would_fail():\n",
    "                    #this is to make the statement fail if it would fail for paper dataframe\n",
    "                    #this cuts of around 15% of the data as we see through testing\n",
    "                    paperId = data[person_index]['papers'][paper_index]['paperId']\n",
    "                    title = data[person_index]['papers'][paper_index]['title']\n",
    "                    year  = data[person_index]['papers'][paper_index]['year']\n",
    "                    doi = data[person_index]['papers'][paper_index]['externalIds']['DOI']\n",
    "                    citationCount = data[person_index]['papers'][paper_index]['citationCount']\n",
    "\n",
    "                    helper1 = []\n",
    "                    for i in range(len(data[person_index]['papers'][paper_index]['s2FieldsOfStudy'])):\n",
    "                        helper1.append(data[person_index]['papers'][paper_index]['s2FieldsOfStudy'][i][\"category\"])\n",
    "\n",
    "                    helper2 = []\n",
    "                    for i in range(len(data[person_index]['papers'][paper_index]['authors'])):\n",
    "                        helper2.append(data[person_index]['papers'][paper_index]['authors'][i]['authorId'])\n",
    "                fail_if_other_dataframe_would_fail()\n",
    "\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "                #actual stuff we need to do in this dataframe\n",
    "                \n",
    "                paperId  =  data[person_index]['papers'][paper_index]['paperId']\n",
    "                abstract  =  data[person_index]['papers'][paper_index]['abstract']\n",
    "                helperdict3['paperId'].append(paperId)\n",
    "                helperdict3['abstract'].append(abstract)\n",
    "            except:\n",
    "                pass\n",
    "    print(g)\n",
    "    return pd.DataFrame(helperdict3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1b67c3",
   "metadata": {},
   "source": [
    "The stuff below is for testing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "id": "568fd8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = BASE_URL + VERSION + RESOURCE\n",
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/batch?\"   #author or paper\n",
    "\n",
    "params = \"fields=aliases,name,papers.title,papers.abstract,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount,papers.authors\"\n",
    "my_url = BASE_URL + VERSION + RESOURCE\n",
    "for i in range(1):\n",
    "    json_data = {'ids':all_authers_list[i*10:i*10 + 10]}\n",
    "    r = requests.post(my_url, json = json_data, params = params)\n",
    "    data = r.json()\n",
    "\n",
    "\n",
    "\n",
    "#authorDataframe = author_df(data)\n",
    "#print(authorDataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "dbfb3590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1135\n",
      "918\n",
      "369\n",
      "527\n",
      "448\n",
      "485\n",
      "832\n",
      "645\n",
      "948\n",
      "842\n",
      "                                       paperId  \\\n",
      "0     6685eb576941d36369c1dda4940a0e1de6d51582   \n",
      "1     b7b8569eb8e841f0170fd690089e81a658c6c6b3   \n",
      "2     1384fb54b8a5c5848e17da1a29a9f710122d222c   \n",
      "3     347e89236ab2f1f1e0fa8b3967f8250c80d8f592   \n",
      "4     a4cdb9cd7b56681784bc101097121c5aee7c6efa   \n",
      "...                                        ...   \n",
      "5889  af05425d0fd3f4fb4c07fa320baa411ebc3fdab1   \n",
      "5890  42bb017d61d7057aeffbcb2771c1ca32e538ab41   \n",
      "5891  39103838fda37dc0145301c9bb478f071e04f65e   \n",
      "5892  63815c0655785919b998c795e7e3d072789591bc   \n",
      "5893  8fa666dfcbb1c849020248716897cf992b7f633e   \n",
      "\n",
      "                                                  title    year  \\\n",
      "0        DIALYSIS. PATHOPHYSIOLOGY AND CLINICAL STUDIES  2014.0   \n",
      "1            Acute renal failure - clinical studies - 2  2009.0   \n",
      "2           Collaborating with care in virtual sessions  2020.0   \n",
      "3                                     Contemporary IMRT  2019.0   \n",
      "4     Developments in IMRT using a multileaf collima...  2019.0   \n",
      "...                                                 ...     ...   \n",
      "5889  Enhancing reliability in an industrial LAN: de...  1990.0   \n",
      "5890  Design and performance evaluation of an optica...  1989.0   \n",
      "5891      Fault tolerance increasing in token ring LANs  1988.0   \n",
      "5892  On the implementation of an optical token-ring...  1988.0   \n",
      "5893  Reliability and fault tolerance aspects in an ...  1988.0   \n",
      "\n",
      "                        externalId  citationCount  \\\n",
      "0               10.1093/NDT/GFU177              0   \n",
      "1          10.1093/NDTPLUS/2.S2.22              0   \n",
      "2            10.17605/OSF.IO/HG3SW              0   \n",
      "3            10.1201/9780429144066              0   \n",
      "4          10.1201/9780429144066-3              0   \n",
      "...                            ...            ...   \n",
      "5889             10.1109/41.103446              2   \n",
      "5890  10.1016/0140-3664(89)90112-6              2   \n",
      "5891        10.1109/LCN.1988.10259              6   \n",
      "5892       10.1109/PCCC.1988.10063              6   \n",
      "5893      10.1109/FTDCS.1988.26715              2   \n",
      "\n",
      "                              fields  \\\n",
      "0                          [Biology]   \n",
      "1                         [Medicine]   \n",
      "2                                 []   \n",
      "3     [Medicine, Physics, Education]   \n",
      "4                          [Physics]   \n",
      "...                              ...   \n",
      "5889                   [Engineering]   \n",
      "5890              [Computer Science]   \n",
      "5891              [Computer Science]   \n",
      "5892                       [Physics]   \n",
      "5893              [Computer Science]   \n",
      "\n",
      "                                              authorIds  \n",
      "0     [4471196, 145690777, 83948336, 3237287, 860021...  \n",
      "1     [2007681691, 34632708, 32590423, 146554107, 47...  \n",
      "2     [3411250, 15856420, 1607624636, 97866793, 1195...  \n",
      "3                                           [145071876]  \n",
      "4                                           [145071876]  \n",
      "...                                                 ...  \n",
      "5889           [1720970, 144324255, 1721699, 145530887]  \n",
      "5890    [1689121, 1720970, 1693329, 2581846, 145530887]  \n",
      "5891                      [1689121, 1720970, 145530887]  \n",
      "5892                      [1689121, 1720970, 145530887]  \n",
      "5893                   [2194201092, 1720970, 145530887]  \n",
      "\n",
      "[5894 rows x 7 columns]\n",
      "                                       paperId  \\\n",
      "0     6685eb576941d36369c1dda4940a0e1de6d51582   \n",
      "1     b7b8569eb8e841f0170fd690089e81a658c6c6b3   \n",
      "2     1384fb54b8a5c5848e17da1a29a9f710122d222c   \n",
      "3     347e89236ab2f1f1e0fa8b3967f8250c80d8f592   \n",
      "4     a4cdb9cd7b56681784bc101097121c5aee7c6efa   \n",
      "...                                        ...   \n",
      "5889  af05425d0fd3f4fb4c07fa320baa411ebc3fdab1   \n",
      "5890  42bb017d61d7057aeffbcb2771c1ca32e538ab41   \n",
      "5891  39103838fda37dc0145301c9bb478f071e04f65e   \n",
      "5892  63815c0655785919b998c795e7e3d072789591bc   \n",
      "5893  8fa666dfcbb1c849020248716897cf992b7f633e   \n",
      "\n",
      "                                               abstract  \n",
      "0                                                  None  \n",
      "1                                                  None  \n",
      "2                                                  None  \n",
      "3     Give us 5 minutes and we will show you the bes...  \n",
      "4                                                  None  \n",
      "...                                                 ...  \n",
      "5889  A fault-tolerant fiber-optic LAN (local area n...  \n",
      "5890                                               None  \n",
      "5891  The authors analyze error-recovery mechanisms ...  \n",
      "5892  A description is given of an MAC (medium acces...  \n",
      "5893  An optical-fiber local area network DAFNE (Det...  \n",
      "\n",
      "[5894 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "BASE_URL = \"https://api.semanticscholar.org/graph/\"\n",
    "VERSION = \"v1/\"\n",
    "RESOURCE = \"author/batch?\"   #author or paper\n",
    "params = \"fields=aliases,name,papers.title,papers.abstract,papers.year,papers.externalIds,papers.s2FieldsOfStudy,papers.citationCount,papers.authors\"\n",
    "k = True\n",
    "\n",
    "for i in range(int(len(all_authers_list)/10)):\n",
    "    my_url = BASE_URL + VERSION + RESOURCE\n",
    "    json_data = {'ids':all_authers_list[i*10:(i*10 )+10]}\n",
    "    r = requests.post(my_url, json = json_data, params = params)\n",
    "    data = r.json()\n",
    "    \n",
    "    \n",
    "    if len(data) <= 1:\n",
    "        pass\n",
    "    else:\n",
    "        if k:\n",
    "            authorDataframe = author_df(data)\n",
    "            paperDataframe = paper_df(data) \n",
    "            abstractDataframe = abstract_df(data)\n",
    "            k = False\n",
    "        else: \n",
    "\n",
    "            new_authorDataframe = author_df(data)\n",
    "            authorDataframe = pd.concat([authorDataframe, new_authorDataframe], ignore_index=True, sort=False)\n",
    "\n",
    "            new_paperDataframe = paper_df(data)\n",
    "            paperDataframe = pd.concat([paperDataframe, new_paperDataframe], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "            new_abstractDatafram = abstract_df(data)\n",
    "            abstractDataframe = pd.concat([abstractDataframe, new_abstractDatafram], ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "\n",
    "            #authorDataframe.concat(author_df(data))\n",
    "            #paperDataframe.concat(paper_df(data))\n",
    "            #abstractDataframe.concat(abstract_df(data))\n",
    "\n",
    "    time.sleep(5*60 / 100 + 0.1)\n",
    "\n",
    "    if i >= 9:\n",
    "        break\n",
    "\n",
    "\n",
    "print(paperDataframe)  \n",
    "print(abstractDataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "b0c89ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      authorId             name  \\\n",
      "0   2080725882      C. A. Beati   \n",
      "1   2073301487          L. Toma   \n",
      "2   2053433279     Sonia Graham   \n",
      "3    145071876          S. Webb   \n",
      "4      2088358   Mario Paolucci   \n",
      "..         ...              ...   \n",
      "95    21778810       K. Paarnio   \n",
      "96  2111477347     David Sionit   \n",
      "97     1923302  R. Angus Silver   \n",
      "98   100716450    Jeff Huizinga   \n",
      "99     1720970       V. Catania   \n",
      "\n",
      "                                              aliases  citationCount field  \n",
      "0                                                None              0  None  \n",
      "1                  [L. Tomá, Luca Di Toma, Luca Toma]              1  None  \n",
      "2                                                None              0  None  \n",
      "3   [S Webb, S. Webb, Steve Webb, Stephen E. D. Webb]          11868  None  \n",
      "4                                       [M. Paolucci]           2038  None  \n",
      "..                                                ...            ...   ...  \n",
      "95         [K Paarnio, K. Paarnio, Karoliina Paarnio]            181  None  \n",
      "96                                               None           1501  None  \n",
      "97                                   [R Angus Silver]            107  None  \n",
      "98                                 [Jeffrey Huizinga]              2  None  \n",
      "99  [V. Catania, V. Rosato Inaf-osservatorio Astro...           3429  None  \n",
      "\n",
      "[100 rows x 5 columns]\n",
      "7149\n"
     ]
    }
   ],
   "source": [
    "print(authorDataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "id": "12951799",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorDataframe.to_csv('authorDF.csv', index=False)\n",
    "paperDataframe.to_csv('paperDF.csv', index=False)\n",
    "abstractDataframe.to_csv('abstractDF.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67996c9",
   "metadata": {},
   "source": [
    "Det vi ved er at vi kan søge efter op til 1000 papers at a time \n",
    "\n",
    "resource = \"author/batch\"\n",
    "\n",
    "spørgsmål.\n",
    "Hvordan skal params se ud ifølge deres API https://api.semanticscholar.org/api-docs/graph#tag/Author-Data/operation/get_graph_get_author_search\n",
    "\n",
    "Husk at fjerne \\n for linjerne via [:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7 mil papers\n",
    "180 thousand authors\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
